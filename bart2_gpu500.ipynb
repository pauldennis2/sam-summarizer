{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6136680",
   "metadata": {},
   "source": [
    "# SAM Summarization MVP\n",
    "\n",
    "This notebook will demonstrate the process of using a pre-trained BART model for summarization of conversations from the SAMSum data set.\n",
    "\n",
    "### Data Loading/Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb80f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "\n",
    "train_df = pd.read_csv('samsum_csv_data/train.csv')\n",
    "validate_df = pd.read_csv('samsum_csv_data/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d756cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer for model: facebook/bart-base...\n",
      "Tokenizer vocabulary size: 50265\n"
     ]
    }
   ],
   "source": [
    "poc_train_df = train_df.head(500)\n",
    "poc_validate_df = validate_df.head(20)\n",
    "\n",
    "# Convert pandas DataFrames to ðŸ¤— HF Dataset objects (ref based)\n",
    "hf_train_dataset = Dataset.from_pandas(poc_train_df)\n",
    "hf_validate_dataset = Dataset.from_pandas(poc_validate_df)\n",
    "\n",
    "MODEL_NAME = \"facebook/bart-base\"\n",
    "\n",
    "print(f\"\\nLoading tokenizer for model: {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74803d9f",
   "metadata": {},
   "source": [
    "### Pre-processing/Tokenization\n",
    "\n",
    "Code for data pre-processing. We'll set a relatively long max input size and create a warning if conversations are truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b048bf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing training/validation data (tokenizing and aligning lengths)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae78e77ee8bc4343b1e9147e204b8f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b4dbd97d714bf791a22f155123b845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In token counts (effectively word)\n",
    "MAX_INPUT_LENGTH = 1024 # Some dialogs will likely exceed this\n",
    "MAX_TARGET_LENGTH = 128 # If our summary is trying to go longer than this it's wrong\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"dialogue\"],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True\n",
    "    )\n",
    "\n",
    "    tokenized_labels = tokenizer(\n",
    "        text_target=examples[\"summary\"],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    for i, length in enumerate(tokenized_inputs[\"length\"]):\n",
    "        if length == MAX_INPUT_LENGTH and \"overflowing_tokens\" in tokenized_inputs:\n",
    "            overflow = tokenized_inputs[\"overflowing_tokens\"][i] if i < len(tokenized_inputs[\"overflowing_tokens\"]) else []\n",
    "            overflow_text = tokenizer.decode(overflow, skip_special_tokens=True)\n",
    "            print(f\"âš ï¸ Truncated example {i}: {overflow_text[:100]}...\")\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = tokenized_labels[\"input_ids\"]\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "print(f\"\\nPreprocessing training/validation data (tokenizing and aligning lengths)...\")\n",
    "tokenized_hf_train_dataset = hf_train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True, # Process examples in batches for speed\n",
    "    remove_columns=['id', 'dialogue', 'summary'] # Remove original text columns\n",
    ")\n",
    "\n",
    "tokenized_hf_validate_dataset = hf_validate_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['id', 'dialogue', 'summary']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39156601",
   "metadata": {},
   "source": [
    "### Training Parameters\n",
    "\n",
    "We are optimizing these training parameters to be GPU-memory-friendly at the cost of performance hits in other areas. `fp16=True, gradient_checkpointing=True` were tried elsewhere and did not allow scaling to 1000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7eb55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading BART model for sequence-to-sequence: facebook/bart-base...\n",
      "Model loaded successfully.\n",
      "Using device: cuda\n",
      "\n",
      "Setting up Training Arguments. Output directory: ./bart_samsum_poc_results\n",
      "\n",
      "Training Arguments configured.\n"
     ]
    }
   ],
   "source": [
    "tokenized_hf_train_dataset.set_format(\"torch\")\n",
    "tokenized_hf_validate_dataset.set_format(\"torch\")\n",
    "\n",
    "print(f\"\\nLoading BART model for sequence-to-sequence: {MODEL_NAME}...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "output_dir = \"./bart_samsum_poc_results\"\n",
    "print(f\"\\nSetting up Training Arguments. Output directory: {output_dir}\")\n",
    "# Includes some default values like num_train_epochs for future clarity\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=2,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    report_to=\"none\",\n",
    "    gradient_accumulation_steps=2, # Memory issues\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\", # Look for higher ROUGE-1 scores \n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Arguments configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1fdd65",
   "metadata": {},
   "source": [
    "### Training Metrics and Initialization\n",
    "\n",
    "As instructed, we'll use ROUGE scores - a measure of word overlap between our target \"reference\" summary and our model's generated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e534d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading ROUGE metric...\n",
      "Metric computation function defined.\n",
      "\n",
      "Initializing the Trainer...\n",
      "Trainer initialized successfully. Ready for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7257/288411429.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading ROUGE metric...\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    predictions = predictions[0]\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Post-process for ROUGE: remove extra whitespace and newlines\n",
    "    decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n",
    "\n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "print(\"Metric computation function defined.\")\n",
    "\n",
    "print(\"\\nInitializing the Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_hf_train_dataset,\n",
    "    eval_dataset=tokenized_hf_validate_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully. Ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd544ca3",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Choose your own training montage music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "343a07d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 12:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.204600</td>\n",
       "      <td>10.251661</td>\n",
       "      <td>42.117600</td>\n",
       "      <td>21.862000</td>\n",
       "      <td>39.190100</td>\n",
       "      <td>42.160200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>8.172100</td>\n",
       "      <td>6.499501</td>\n",
       "      <td>28.307100</td>\n",
       "      <td>15.460400</td>\n",
       "      <td>27.330000</td>\n",
       "      <td>28.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.337400</td>\n",
       "      <td>3.831983</td>\n",
       "      <td>53.516400</td>\n",
       "      <td>27.167200</td>\n",
       "      <td>51.186200</td>\n",
       "      <td>53.611200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.582900</td>\n",
       "      <td>2.712469</td>\n",
       "      <td>55.076200</td>\n",
       "      <td>27.645000</td>\n",
       "      <td>51.537600</td>\n",
       "      <td>55.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.632200</td>\n",
       "      <td>1.768104</td>\n",
       "      <td>56.127500</td>\n",
       "      <td>29.986300</td>\n",
       "      <td>54.374600</td>\n",
       "      <td>56.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.783200</td>\n",
       "      <td>1.063573</td>\n",
       "      <td>59.022900</td>\n",
       "      <td>36.889200</td>\n",
       "      <td>57.750200</td>\n",
       "      <td>59.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.191700</td>\n",
       "      <td>0.648764</td>\n",
       "      <td>56.315700</td>\n",
       "      <td>32.849800</td>\n",
       "      <td>54.723100</td>\n",
       "      <td>56.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.965300</td>\n",
       "      <td>0.462819</td>\n",
       "      <td>58.052000</td>\n",
       "      <td>35.492800</td>\n",
       "      <td>56.658800</td>\n",
       "      <td>58.130400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.830200</td>\n",
       "      <td>0.383688</td>\n",
       "      <td>58.753100</td>\n",
       "      <td>33.859600</td>\n",
       "      <td>57.085500</td>\n",
       "      <td>58.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.535100</td>\n",
       "      <td>0.396347</td>\n",
       "      <td>53.088300</td>\n",
       "      <td>31.047400</td>\n",
       "      <td>51.459700</td>\n",
       "      <td>53.102200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.494600</td>\n",
       "      <td>0.398680</td>\n",
       "      <td>54.213200</td>\n",
       "      <td>31.315900</td>\n",
       "      <td>53.426200</td>\n",
       "      <td>54.328200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.566800</td>\n",
       "      <td>0.377202</td>\n",
       "      <td>55.790000</td>\n",
       "      <td>31.562800</td>\n",
       "      <td>54.317800</td>\n",
       "      <td>55.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.741900</td>\n",
       "      <td>0.366729</td>\n",
       "      <td>54.036800</td>\n",
       "      <td>27.658200</td>\n",
       "      <td>51.620000</td>\n",
       "      <td>53.896200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.477300</td>\n",
       "      <td>0.352637</td>\n",
       "      <td>55.022400</td>\n",
       "      <td>30.879500</td>\n",
       "      <td>52.831500</td>\n",
       "      <td>55.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.523100</td>\n",
       "      <td>0.351638</td>\n",
       "      <td>55.885100</td>\n",
       "      <td>29.914800</td>\n",
       "      <td>54.198300</td>\n",
       "      <td>55.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.508000</td>\n",
       "      <td>0.342827</td>\n",
       "      <td>58.947500</td>\n",
       "      <td>33.381700</td>\n",
       "      <td>56.720700</td>\n",
       "      <td>59.120900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.436500</td>\n",
       "      <td>0.355027</td>\n",
       "      <td>59.612400</td>\n",
       "      <td>33.558600</td>\n",
       "      <td>57.661700</td>\n",
       "      <td>59.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.473500</td>\n",
       "      <td>0.358937</td>\n",
       "      <td>55.629600</td>\n",
       "      <td>29.045400</td>\n",
       "      <td>54.159200</td>\n",
       "      <td>55.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.472600</td>\n",
       "      <td>0.356036</td>\n",
       "      <td>57.009600</td>\n",
       "      <td>32.379600</td>\n",
       "      <td>54.446500</td>\n",
       "      <td>57.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.403900</td>\n",
       "      <td>0.339147</td>\n",
       "      <td>58.942900</td>\n",
       "      <td>31.942000</td>\n",
       "      <td>56.449200</td>\n",
       "      <td>58.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.375400</td>\n",
       "      <td>0.346142</td>\n",
       "      <td>57.875000</td>\n",
       "      <td>30.311900</td>\n",
       "      <td>55.680500</td>\n",
       "      <td>58.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.599100</td>\n",
       "      <td>0.337512</td>\n",
       "      <td>58.874900</td>\n",
       "      <td>33.181900</td>\n",
       "      <td>56.731100</td>\n",
       "      <td>58.853800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.344609</td>\n",
       "      <td>55.695200</td>\n",
       "      <td>27.202800</td>\n",
       "      <td>53.347700</td>\n",
       "      <td>55.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.393200</td>\n",
       "      <td>0.342472</td>\n",
       "      <td>55.332000</td>\n",
       "      <td>29.468400</td>\n",
       "      <td>53.801800</td>\n",
       "      <td>55.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.345909</td>\n",
       "      <td>57.264800</td>\n",
       "      <td>31.251100</td>\n",
       "      <td>54.816700</td>\n",
       "      <td>57.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.343400</td>\n",
       "      <td>0.337467</td>\n",
       "      <td>56.238800</td>\n",
       "      <td>31.253200</td>\n",
       "      <td>53.729700</td>\n",
       "      <td>56.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.404300</td>\n",
       "      <td>0.339751</td>\n",
       "      <td>56.794500</td>\n",
       "      <td>31.821200</td>\n",
       "      <td>54.976600</td>\n",
       "      <td>56.930800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.301700</td>\n",
       "      <td>0.339233</td>\n",
       "      <td>60.214900</td>\n",
       "      <td>33.975300</td>\n",
       "      <td>57.509500</td>\n",
       "      <td>60.381200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.489000</td>\n",
       "      <td>0.336193</td>\n",
       "      <td>59.923900</td>\n",
       "      <td>31.717600</td>\n",
       "      <td>56.987700</td>\n",
       "      <td>60.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.284400</td>\n",
       "      <td>0.333541</td>\n",
       "      <td>60.238000</td>\n",
       "      <td>32.130300</td>\n",
       "      <td>56.995400</td>\n",
       "      <td>60.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.389200</td>\n",
       "      <td>0.327603</td>\n",
       "      <td>59.830500</td>\n",
       "      <td>33.322200</td>\n",
       "      <td>57.027300</td>\n",
       "      <td>59.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.412200</td>\n",
       "      <td>0.326495</td>\n",
       "      <td>56.448500</td>\n",
       "      <td>31.931800</td>\n",
       "      <td>54.206700</td>\n",
       "      <td>56.557300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.369600</td>\n",
       "      <td>0.330196</td>\n",
       "      <td>58.486300</td>\n",
       "      <td>32.802700</td>\n",
       "      <td>56.038800</td>\n",
       "      <td>58.632700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.223900</td>\n",
       "      <td>0.339092</td>\n",
       "      <td>59.749600</td>\n",
       "      <td>34.358900</td>\n",
       "      <td>57.634200</td>\n",
       "      <td>59.916200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.489900</td>\n",
       "      <td>0.342241</td>\n",
       "      <td>58.776200</td>\n",
       "      <td>33.938000</td>\n",
       "      <td>56.640500</td>\n",
       "      <td>59.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.451600</td>\n",
       "      <td>0.342165</td>\n",
       "      <td>58.974500</td>\n",
       "      <td>34.333900</td>\n",
       "      <td>56.747500</td>\n",
       "      <td>59.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.340293</td>\n",
       "      <td>57.967000</td>\n",
       "      <td>32.589400</td>\n",
       "      <td>56.614000</td>\n",
       "      <td>58.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.348400</td>\n",
       "      <td>0.341754</td>\n",
       "      <td>61.310000</td>\n",
       "      <td>38.105200</td>\n",
       "      <td>59.276100</td>\n",
       "      <td>61.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.280600</td>\n",
       "      <td>0.346024</td>\n",
       "      <td>58.520500</td>\n",
       "      <td>33.138000</td>\n",
       "      <td>56.791700</td>\n",
       "      <td>58.701300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.417400</td>\n",
       "      <td>0.345114</td>\n",
       "      <td>56.698100</td>\n",
       "      <td>31.607900</td>\n",
       "      <td>55.368700</td>\n",
       "      <td>56.816300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.343611</td>\n",
       "      <td>59.065400</td>\n",
       "      <td>35.344800</td>\n",
       "      <td>57.680100</td>\n",
       "      <td>59.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.327600</td>\n",
       "      <td>0.335400</td>\n",
       "      <td>59.783700</td>\n",
       "      <td>36.550800</td>\n",
       "      <td>57.588400</td>\n",
       "      <td>60.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.331992</td>\n",
       "      <td>61.025100</td>\n",
       "      <td>36.747600</td>\n",
       "      <td>58.896100</td>\n",
       "      <td>61.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>0.333245</td>\n",
       "      <td>60.316200</td>\n",
       "      <td>35.766200</td>\n",
       "      <td>57.882100</td>\n",
       "      <td>60.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.420300</td>\n",
       "      <td>0.332804</td>\n",
       "      <td>59.696000</td>\n",
       "      <td>32.558700</td>\n",
       "      <td>56.535700</td>\n",
       "      <td>59.822200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.406700</td>\n",
       "      <td>0.326386</td>\n",
       "      <td>61.403100</td>\n",
       "      <td>33.925100</td>\n",
       "      <td>58.710400</td>\n",
       "      <td>61.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.356200</td>\n",
       "      <td>0.328126</td>\n",
       "      <td>61.638700</td>\n",
       "      <td>35.381100</td>\n",
       "      <td>59.063400</td>\n",
       "      <td>61.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.325700</td>\n",
       "      <td>0.326121</td>\n",
       "      <td>61.552200</td>\n",
       "      <td>35.701800</td>\n",
       "      <td>59.294100</td>\n",
       "      <td>61.772600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.355300</td>\n",
       "      <td>0.327108</td>\n",
       "      <td>61.282800</td>\n",
       "      <td>34.526400</td>\n",
       "      <td>58.663600</td>\n",
       "      <td>61.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>0.327319</td>\n",
       "      <td>61.684400</td>\n",
       "      <td>36.503300</td>\n",
       "      <td>59.475300</td>\n",
       "      <td>62.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.298800</td>\n",
       "      <td>0.325601</td>\n",
       "      <td>62.106200</td>\n",
       "      <td>37.642900</td>\n",
       "      <td>60.398800</td>\n",
       "      <td>62.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.188300</td>\n",
       "      <td>0.328458</td>\n",
       "      <td>62.534200</td>\n",
       "      <td>37.475700</td>\n",
       "      <td>59.951000</td>\n",
       "      <td>62.517900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.335700</td>\n",
       "      <td>0.329913</td>\n",
       "      <td>61.324600</td>\n",
       "      <td>35.802200</td>\n",
       "      <td>58.585500</td>\n",
       "      <td>61.384900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.328233</td>\n",
       "      <td>61.024600</td>\n",
       "      <td>36.079900</td>\n",
       "      <td>58.604100</td>\n",
       "      <td>61.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.328846</td>\n",
       "      <td>62.613700</td>\n",
       "      <td>36.760800</td>\n",
       "      <td>59.633500</td>\n",
       "      <td>62.553900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.229500</td>\n",
       "      <td>0.329535</td>\n",
       "      <td>61.728300</td>\n",
       "      <td>36.157400</td>\n",
       "      <td>59.392200</td>\n",
       "      <td>61.915200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.296300</td>\n",
       "      <td>0.327773</td>\n",
       "      <td>61.771400</td>\n",
       "      <td>35.273600</td>\n",
       "      <td>59.245800</td>\n",
       "      <td>61.862300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.253300</td>\n",
       "      <td>0.328693</td>\n",
       "      <td>63.349500</td>\n",
       "      <td>37.365300</td>\n",
       "      <td>60.736100</td>\n",
       "      <td>63.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.257600</td>\n",
       "      <td>0.330385</td>\n",
       "      <td>61.916400</td>\n",
       "      <td>36.449000</td>\n",
       "      <td>59.977900</td>\n",
       "      <td>62.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.295800</td>\n",
       "      <td>0.329076</td>\n",
       "      <td>63.872800</td>\n",
       "      <td>38.322100</td>\n",
       "      <td>61.656200</td>\n",
       "      <td>64.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.330839</td>\n",
       "      <td>62.579400</td>\n",
       "      <td>37.954800</td>\n",
       "      <td>60.506300</td>\n",
       "      <td>62.728600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.248100</td>\n",
       "      <td>0.330040</td>\n",
       "      <td>62.806000</td>\n",
       "      <td>37.699400</td>\n",
       "      <td>60.498100</td>\n",
       "      <td>62.847400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.328774</td>\n",
       "      <td>62.834800</td>\n",
       "      <td>36.457800</td>\n",
       "      <td>59.727700</td>\n",
       "      <td>62.868800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.327856</td>\n",
       "      <td>63.468300</td>\n",
       "      <td>37.128500</td>\n",
       "      <td>60.351100</td>\n",
       "      <td>63.452300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.266900</td>\n",
       "      <td>0.329627</td>\n",
       "      <td>63.560900</td>\n",
       "      <td>37.896900</td>\n",
       "      <td>60.657900</td>\n",
       "      <td>63.636500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.330800</td>\n",
       "      <td>0.331030</td>\n",
       "      <td>62.374000</td>\n",
       "      <td>36.756000</td>\n",
       "      <td>60.234900</td>\n",
       "      <td>62.411500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.391200</td>\n",
       "      <td>0.332124</td>\n",
       "      <td>61.170700</td>\n",
       "      <td>35.001100</td>\n",
       "      <td>59.082900</td>\n",
       "      <td>61.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.163200</td>\n",
       "      <td>0.333390</td>\n",
       "      <td>61.725000</td>\n",
       "      <td>35.201600</td>\n",
       "      <td>59.599100</td>\n",
       "      <td>61.583200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.225200</td>\n",
       "      <td>0.333895</td>\n",
       "      <td>60.593300</td>\n",
       "      <td>34.883200</td>\n",
       "      <td>58.288000</td>\n",
       "      <td>60.621300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.313900</td>\n",
       "      <td>0.334149</td>\n",
       "      <td>59.866700</td>\n",
       "      <td>34.957800</td>\n",
       "      <td>57.508400</td>\n",
       "      <td>59.839100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.221600</td>\n",
       "      <td>0.334060</td>\n",
       "      <td>60.215200</td>\n",
       "      <td>34.605400</td>\n",
       "      <td>57.971400</td>\n",
       "      <td>60.206900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.287900</td>\n",
       "      <td>0.333698</td>\n",
       "      <td>60.431100</td>\n",
       "      <td>34.605400</td>\n",
       "      <td>57.971400</td>\n",
       "      <td>60.385600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.352600</td>\n",
       "      <td>0.333125</td>\n",
       "      <td>60.330800</td>\n",
       "      <td>34.661400</td>\n",
       "      <td>57.852200</td>\n",
       "      <td>60.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.250500</td>\n",
       "      <td>0.332770</td>\n",
       "      <td>60.330800</td>\n",
       "      <td>34.889500</td>\n",
       "      <td>58.035900</td>\n",
       "      <td>60.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.174600</td>\n",
       "      <td>0.332633</td>\n",
       "      <td>60.330800</td>\n",
       "      <td>34.661400</td>\n",
       "      <td>57.852200</td>\n",
       "      <td>60.284000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniconda3/envs/pytorch_env/lib/python3.10/site-packages/transformers/modeling_utils.py:3465: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               =   851791GF\n",
      "  train_loss               =     0.8301\n",
      "  train_runtime            = 0:12:19.14\n",
      "  train_samples_per_second =      2.029\n",
      "  train_steps_per_second   =      1.015\n",
      "\n",
      "Model and tokenizer saved to: ./bart_samsum_poc_results\n",
      "Training metrics logged and saved.\n",
      "\n",
      "Running final evaluation on the validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_loss               =     0.3291\n",
      "  eval_rouge1             =    63.8728\n",
      "  eval_rouge2             =    38.3221\n",
      "  eval_rougeL             =    61.6562\n",
      "  eval_rougeLsum          =    64.0096\n",
      "  eval_runtime            = 0:00:01.83\n",
      "  eval_samples_per_second =     10.899\n",
      "  eval_steps_per_second   =      5.449\n",
      "Final evaluation complete.\n",
      "{'eval_loss': 0.32907551527023315, 'eval_rouge1': 63.8728, 'eval_rouge2': 38.3221, 'eval_rougeL': 61.6562, 'eval_rougeLsum': 64.0096, 'eval_runtime': 1.8351, 'eval_samples_per_second': 10.899, 'eval_steps_per_second': 5.449, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Start the training process\n",
    "print(\"\\nStarting model training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "trainer.save_model() # Saves the model and tokenizer to the output_dir specified in TrainingArguments\n",
    "# For good measure, you can also save the tokenizer explicitly if desired (though save_model usually handles it)\n",
    "# tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "# Save training metrics (optional, but good for tracking progress)\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "print(f\"\\nModel and tokenizer saved to: {training_args.output_dir}\")\n",
    "print(\"Training metrics logged and saved.\")\n",
    "\n",
    "# Optionally, you can also run a final evaluation on the validation set after training\n",
    "print(\"\\nRunning final evaluation on the validation set...\")\n",
    "eval_metrics = trainer.evaluate(eval_dataset=tokenized_hf_validate_dataset)\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "print(\"Final evaluation complete.\")\n",
    "print(eval_metrics) # Print the evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9cadb6",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The model maintained its performance on conversations it hasn't already seen, indicating that overfitting us not a major concern presently. Let's take a look at some examples next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68388388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading fine-tuned model and tokenizer from ./bart_samsum_poc_results...\n",
      "Using device: cpu\n",
      "Model and tokenizer loaded successfully for inference.\n",
      "\n",
      "--- Model Inference Examples ---\n",
      "\n",
      "--- Example 1 ---\n",
      "Original Dialogue:\n",
      "Edd: wow, did you hear that they're transferring us to a different department?\n",
      "Rose: whaaaaat :o\n",
      "Rose: no! where'd you hear that?\n",
      "Edd: well, it's quite official\n",
      "Edd: Anderson just told us\n",
      "Rose: and do you know what it changes for us?\n",
      "Edd: they won't change the professors\n",
      "Edd: but i know the paperwork will get trickier\n",
      "Rose: and i guess that is a move that is supposed to make everything easier\n",
      "Edd: yeah, guess so\n",
      "Edd: they have a funny way of understanding 'to make things easier'\n",
      "\n",
      "Reference Summary:\n",
      "Rose and Edd will be transferred to a new department. Their professors will not change but paperwork will become more difficult.\n",
      "\n",
      "Generated Summary:\n",
      "Edd and Rose are transferred to a different department. They will change the professors, but the paperwork will get trickier. They have a funny way of understanding that.\n",
      "------------------------------\n",
      "\n",
      "--- Example 2 ---\n",
      "Original Dialogue:\n",
      "Tom: Where is the \"Sala del Capitolo\"\n",
      "Kevin: it's in the main building\n",
      "Martin: The one with the huge round table\n",
      "Tom: ok! I know.\n",
      "Tom: Thx\n",
      "\n",
      "Reference Summary:\n",
      "\"Sala del Capitolo\" Tom is looking for is in the main building.\n",
      "\n",
      "Generated Summary:\n",
      "Tom and Martin are looking for the \"Sala del Capitolo\" in the main building.   \n",
      "\n",
      "------------------------------\n",
      "\n",
      "--- Example 3 ---\n",
      "Original Dialogue:\n",
      "Patricia: The rowing practice is cancelled!\n",
      "Kate: Why? \n",
      "Lindsey: What a shame... \n",
      "Lindsey: I was really looking forward\n",
      "Patricia: I just got this email\n",
      "Patricia: <photo_file>\n",
      "Kate: I also got it\n",
      "Kate: I just saw it now\n",
      "Patricia: Few of the members have injuries, some are sick\n",
      "Patricia: And we cannot row without so many people absent\n",
      "Kate: It must be also the time of the year\n",
      "Kate: Lots of people are sick\n",
      "Kate: Even in my college\n",
      "Kate: It's insane\n",
      "\n",
      "Reference Summary:\n",
      "The rowing practice is cancelled. A few members have injuries and some are sick. People are sick in Kate's college.\n",
      "\n",
      "Generated Summary:\n",
      "Patricia's rowing practice is cancelled. Kate has just got an email from the rowing club. She is very upset about it.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the output directory where your model was saved\n",
    "output_dir = \"./bart_samsum_poc_results\"\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "print(f\"\\nLoading fine-tuned model and tokenizer from {output_dir}...\")\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "loaded_model = AutoModelForSeq2SeqLM.from_pretrained(output_dir).to(device)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully for inference.\")\n",
    "\n",
    "num_examples_to_show = 3\n",
    "\n",
    "example_data = validate_df.sample(n=num_examples_to_show,\n",
    "                                   random_state=42).to_dict('records')\n",
    "\n",
    "\n",
    "print(\"\\n--- Model Inference Examples ---\")\n",
    "for i, example in enumerate(example_data):\n",
    "    dialogue = example['dialogue']\n",
    "    reference_summary = example['summary']\n",
    "\n",
    "    # Tokenize the input dialogue\n",
    "    inputs = loaded_tokenizer(\n",
    "        dialogue,\n",
    "        return_tensors=\"pt\", # Return PyTorch tensors\n",
    "        max_length=MAX_INPUT_LENGTH, # Use the same max length as training\n",
    "        truncation=True\n",
    "    ).to(device) # Move inputs to the correct device (CPU/GPU)\n",
    "\n",
    "    # Generate summary\n",
    "    # Using parameters that typically work well for summarization\n",
    "    summary_ids = loaded_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        num_beams=4,        # Use beam search for better quality summaries\n",
    "        max_length=MAX_TARGET_LENGTH, # Max length of generated summary\n",
    "        min_length=30,      # Minimum length to encourage more detailed summaries\n",
    "        early_stopping=True, # Stop generation when all beam hypotheses are complete\n",
    "        length_penalty=2.0  # Encourage longer summaries (common for abstractive summarization)\n",
    "    )\n",
    "\n",
    "    # Decode the generated summary IDs back to text\n",
    "    generated_summary = loaded_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Original Dialogue:\\n{dialogue}\")\n",
    "    print(f\"\\nReference Summary:\\n{reference_summary}\")\n",
    "    print(f\"\\nGenerated Summary:\\n{generated_summary}\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
