{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b1bf19",
   "metadata": {},
   "source": [
    "title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a711a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('samsum_csv_data/train.csv')\n",
    "validate_df = pd.read_csv('samsum_csv_data/validation.csv')\n",
    "\n",
    "poc_train_df = train_df.head(200)\n",
    "poc_validate_df = validate_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad6dec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniconda3/envs/pytorch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hugging Face Train Dataset size: 200\n",
      "Hugging Face Validation Dataset size: 40\n",
      "\n",
      "Loading tokenizer for model: t5-small...\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset # Import the Dataset class\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Dataset objects\n",
    "# This is a lightweight conversion, doesn't copy data unnecessarily\n",
    "hf_train_dataset = Dataset.from_pandas(poc_train_df)\n",
    "hf_validate_dataset = Dataset.from_pandas(poc_validate_df)\n",
    "\n",
    "print(f\"\\nHugging Face Train Dataset size: {len(hf_train_dataset)}\")\n",
    "print(f\"Hugging Face Validation Dataset size: {len(hf_validate_dataset)}\")\n",
    "\n",
    "# Define the model name for the tokenizer (using 'bart-base' for efficiency)\n",
    "MODEL_NAME = \"t5-small\"\n",
    "\n",
    "# Load the tokenizer. This downloads the vocabulary and tokenization rules.\n",
    "print(f\"\\nLoading tokenizer for model: {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7fe112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing training data (tokenizing and aligning lengths)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 3076.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 3445.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data preprocessing complete.\n",
      "Sample of tokenized training data structure:\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Input IDs length: 1024\n",
      "Labels length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming MAX_INPUT_LENGTH and MAX_TARGET_LENGTH are already defined\n",
    "# Consider if T5-small fits 1024. For a very small model, 512 might be more common,\n",
    "# but 1024 is generally fine if memory allows with batch size 1.\n",
    "MAX_INPUT_LENGTH = 1024 # BART's typical maximum input length\n",
    "MAX_TARGET_LENGTH = 128 # Reasonable maximum length for summaries\n",
    "def preprocess_function(examples):\n",
    "    # --- IMPORTANT T5 ADJUSTMENT: Add a task prefix to the input dialogue ---\n",
    "    # T5 models expect a task prefix like \"summarize: \" for summarization tasks.\n",
    "    inputs_with_prefix = [f\"summarize: {dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs_with_prefix, # Use the dialogues with the T5 prefix\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"summary\"],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(f\"\\nPreprocessing training data (tokenizing and aligning lengths)...\")\n",
    "tokenized_hf_train_dataset = hf_train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True, # Process examples in batches for speed\n",
    "    remove_columns=['id', 'dialogue', 'summary'] # Remove original text columns\n",
    ")\n",
    "\n",
    "print(f\"Preprocessing validation data...\")\n",
    "tokenized_hf_validate_dataset = hf_validate_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['id', 'dialogue', 'summary']\n",
    ")\n",
    "\n",
    "print(\"\\nData preprocessing complete.\")\n",
    "print(\"Sample of tokenized training data structure:\")\n",
    "print(tokenized_hf_train_dataset[0].keys()) # Show what keys are now in the dataset\n",
    "print(f\"Input IDs length: {len(tokenized_hf_train_dataset[0]['input_ids'])}\")\n",
    "print(f\"Labels length: {len(tokenized_hf_train_dataset[0]['labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f7d30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading t5 model for sequence-to-sequence: t5-small...\n",
      "Model device after loading: cpu\n",
      "Model loaded successfully.\n",
      "\n",
      "Setting up Training Arguments. Output directory: ./t5_samsum_poc_results\n",
      "\n",
      "Training Arguments configured.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "# step 4\n",
    "# Ensure the dataset format is set to PyTorch tensors\n",
    "# This is crucial before passing to the Trainer\n",
    "tokenized_hf_train_dataset.set_format(\"torch\")\n",
    "tokenized_hf_validate_dataset.set_format(\"torch\")\n",
    "\n",
    "# Load the pre-trained BART model for sequence-to-sequence tasks\n",
    "# This downloads the model weights and architecture for 'bart-base'\n",
    "MODEL_NAME = \"t5-small\" # Using the same model name as for tokenizer\n",
    "print(f\"\\nLoading t5 model for sequence-to-sequence: {MODEL_NAME}...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(\"cpu\")\n",
    "print(f\"Model device after loading: {model.device}\") # <-- ADD THIS LINE\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Configure Training Arguments\n",
    "# These define how the training will proceed (epochs, batch size, logging, etc.)\n",
    "output_dir = \"./t5_samsum_poc_results\" # Directory to save model checkpoints and logs\n",
    "print(f\"\\nSetting up Training Arguments. Output directory: {output_dir}\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,                  # Number of training epochs (small for POC)\n",
    "    per_device_train_batch_size=1,       # Batch size per GPU/CPU for training\n",
    "    per_device_eval_batch_size=2,        # Batch size per GPU/CPU for evaluation\n",
    "    warmup_steps=10,                     # Number of steps for learning rate warmup\n",
    "    weight_decay=0.01,                   # L2 regularization to prevent overfitting\n",
    "    logging_dir=f\"{output_dir}/logs\",   # Directory for TensorBoard logs\n",
    "    logging_steps=5,                     # Log training metrics every N steps\n",
    "    eval_strategy=\"steps\",               # Evaluate every N steps \n",
    "    eval_steps=10,                       # How often to run evaluation \n",
    "    save_steps=10,                       # How often to save model checkpoints \n",
    "    report_to=\"none\",                    # Do not report to external services like Weights & Biases \n",
    "    gradient_accumulation_steps=4,       # Accumulate gradients over N steps \n",
    "    # predict_with_generate is NOT a TrainingArguments parameter in your version,\n",
    "    # it will be passed directly to the Trainer.\n",
    "    load_best_model_at_end=True,         # Load the best model found during training \n",
    "    metric_for_best_model=\"rouge1\",      # Metric to monitor for best model selection \n",
    "    greater_is_better=True,              # Higher ROUGE-1 is better \n",
    ")\n",
    "\n",
    "print(\"\\nTraining Arguments configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0383ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading ROUGE metric...\n",
      "Metric computation function defined.\n",
      "Sample train input_ids device: cpu\n",
      "\n",
      "Initializing the Trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3914/3251359790.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully. Ready for training.\n"
     ]
    }
   ],
   "source": [
    "# step 5\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load the ROUGE metric (if not already loaded in the current kernel session)\n",
    "print(\"\\nLoading ROUGE metric...\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # --- IMPORTANT FIX HERE: Handle tuple output from Trainer ---\n",
    "    # If predictions is a tuple, assume the actual logits/generated IDs are the first element.\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    # --- END IMPORTANT FIX ---\n",
    "\n",
    "    # Now, check the shape of predictions. If it's 3D, it's likely logits.\n",
    "    # We need to convert logits to token IDs by taking the argmax.\n",
    "    if predictions.ndim == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Post-process for ROUGE: remove extra whitespace and newlines\n",
    "    decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n",
    "\n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "print(\"Metric computation function defined.\")\n",
    "if len(tokenized_hf_train_dataset) > 0:\n",
    "    sample_batch = tokenized_hf_train_dataset[0]\n",
    "    if isinstance(sample_batch, dict):\n",
    "        # Check a tensor from the sample batch (e.g., 'input_ids')\n",
    "        if 'input_ids' in sample_batch and isinstance(sample_batch['input_ids'], torch.Tensor):\n",
    "            print(f\"Sample train input_ids device: {sample_batch['input_ids'].device}\")\n",
    "        else:\n",
    "            print(\"Sample batch does not contain 'input_ids' tensor, or it's not a tensor.\")\n",
    "    else:\n",
    "        print(\"Sample batch is not a dictionary.\")\n",
    "else:\n",
    "    print(\"Training dataset is empty, cannot check sample batch device.\")\n",
    "\n",
    "# Initialize the Hugging Face Trainer (this part remains the same as it worked last time)\n",
    "print(\"\\nInitializing the Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_hf_train_dataset,\n",
    "    eval_dataset=tokenized_hf_validate_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully. Ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8b46bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 02:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.651900</td>\n",
       "      <td>10.089927</td>\n",
       "      <td>24.604700</td>\n",
       "      <td>11.476200</td>\n",
       "      <td>23.069100</td>\n",
       "      <td>24.759300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.386000</td>\n",
       "      <td>3.337876</td>\n",
       "      <td>34.817100</td>\n",
       "      <td>15.942000</td>\n",
       "      <td>32.776300</td>\n",
       "      <td>35.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.097300</td>\n",
       "      <td>0.693238</td>\n",
       "      <td>42.176600</td>\n",
       "      <td>19.419200</td>\n",
       "      <td>40.521600</td>\n",
       "      <td>42.447100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.490000</td>\n",
       "      <td>0.610536</td>\n",
       "      <td>43.012300</td>\n",
       "      <td>19.288700</td>\n",
       "      <td>41.038800</td>\n",
       "      <td>43.124700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.309600</td>\n",
       "      <td>0.612721</td>\n",
       "      <td>43.998600</td>\n",
       "      <td>18.800700</td>\n",
       "      <td>41.194200</td>\n",
       "      <td>44.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>0.598194</td>\n",
       "      <td>45.162500</td>\n",
       "      <td>19.885000</td>\n",
       "      <td>41.660800</td>\n",
       "      <td>45.418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.729100</td>\n",
       "      <td>0.577926</td>\n",
       "      <td>45.474900</td>\n",
       "      <td>19.971300</td>\n",
       "      <td>41.501900</td>\n",
       "      <td>45.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.743100</td>\n",
       "      <td>0.551439</td>\n",
       "      <td>46.748700</td>\n",
       "      <td>22.189300</td>\n",
       "      <td>42.453300</td>\n",
       "      <td>46.916900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.766400</td>\n",
       "      <td>0.528148</td>\n",
       "      <td>46.308900</td>\n",
       "      <td>21.706800</td>\n",
       "      <td>42.046200</td>\n",
       "      <td>46.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.773700</td>\n",
       "      <td>0.510219</td>\n",
       "      <td>46.731700</td>\n",
       "      <td>21.264900</td>\n",
       "      <td>42.836900</td>\n",
       "      <td>46.824600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.797100</td>\n",
       "      <td>0.503992</td>\n",
       "      <td>46.856700</td>\n",
       "      <td>21.476700</td>\n",
       "      <td>43.062600</td>\n",
       "      <td>47.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.644200</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>47.146000</td>\n",
       "      <td>21.669500</td>\n",
       "      <td>43.297600</td>\n",
       "      <td>47.198400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.496573</td>\n",
       "      <td>47.739500</td>\n",
       "      <td>22.701100</td>\n",
       "      <td>43.975800</td>\n",
       "      <td>47.840400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.793700</td>\n",
       "      <td>0.494208</td>\n",
       "      <td>47.771900</td>\n",
       "      <td>22.885900</td>\n",
       "      <td>43.897500</td>\n",
       "      <td>47.810400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.722100</td>\n",
       "      <td>0.493175</td>\n",
       "      <td>48.027200</td>\n",
       "      <td>23.166900</td>\n",
       "      <td>44.208100</td>\n",
       "      <td>48.068300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               =   151256GF\n",
      "  train_loss               =     2.0756\n",
      "  train_runtime            = 0:02:21.27\n",
      "  train_samples_per_second =      4.247\n",
      "  train_steps_per_second   =      1.062\n",
      "\n",
      "Model and tokenizer saved to: ./t5_samsum_poc_results\n",
      "Training metrics logged and saved.\n",
      "\n",
      "Running final evaluation on the validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_loss               =     0.4932\n",
      "  eval_rouge1             =    48.0272\n",
      "  eval_rouge2             =    23.1669\n",
      "  eval_rougeL             =    44.2081\n",
      "  eval_rougeLsum          =    48.0683\n",
      "  eval_runtime            = 0:00:02.13\n",
      "  eval_samples_per_second =     18.715\n",
      "  eval_steps_per_second   =      9.358\n",
      "Final evaluation complete.\n",
      "{'eval_loss': 0.49317502975463867, 'eval_rouge1': 48.0272, 'eval_rouge2': 23.1669, 'eval_rougeL': 44.2081, 'eval_rougeLsum': 48.0683, 'eval_runtime': 2.1373, 'eval_samples_per_second': 18.715, 'eval_steps_per_second': 9.358, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "#step6\n",
    "\n",
    "# Start the training process\n",
    "print(\"\\nStarting model training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "trainer.save_model() # Saves the model and tokenizer to the output_dir specified in TrainingArguments\n",
    "# For good measure, you can also save the tokenizer explicitly if desired (though save_model usually handles it)\n",
    "# tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "# Save training metrics (optional, but good for tracking progress)\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "print(f\"\\nModel and tokenizer saved to: {training_args.output_dir}\")\n",
    "print(\"Training metrics logged and saved.\")\n",
    "\n",
    "# Optionally, you can also run a final evaluation on the validation set after training\n",
    "print(\"\\nRunning final evaluation on the validation set...\")\n",
    "eval_metrics = trainer.evaluate(eval_dataset=tokenized_hf_validate_dataset)\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "print(\"Final evaluation complete.\")\n",
    "print(eval_metrics) # Print the evaluation results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
